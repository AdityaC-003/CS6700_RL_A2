{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 0.0005\n",
    "UPDATE_EVERY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n      \n",
    "\n",
    "state = env.reset(seed=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCRNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(MCRNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.output_layer = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.output_layer(x), dim=1)\n",
    "        return output\n",
    "\n",
    "    def get_action(self, state):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = self.forward(state)\n",
    "        chosen_action = np.random.choice(\n",
    "            np.arange(self.action_size), p=action_values.cpu().data.numpy().flatten()\n",
    "        )\n",
    "        log_p = torch.log(action_values.squeeze(0)[chosen_action])\n",
    "        return chosen_action, log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent for Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Agent:\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int, baseline: bool):\n",
    "        \"\"\"Agent-Environment Interaction\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        \"\"\" MC-network \"\"\"\n",
    "        self.mc_network = MCRNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.mc_network.parameters(), lr=LR)\n",
    "\n",
    "        \"\"\" Initialize time step (for updating every UPDATE_EVERY steps) \"\"\"\n",
    "        self.t_step = 0\n",
    "\n",
    "        \"\"\" Collecing log of probabilities for gradient descent update \"\"\"\n",
    "        self.log_probs = []\n",
    "\n",
    "        \"\"\" Storing rewards for updates \"\"\"\n",
    "        self.rewards_list = []\n",
    "\n",
    "        \"\"\" Baseline flag, if true, normalize with Value function obtained by TD(0) \"\"\"\n",
    "        self.baseline = baseline\n",
    "        \n",
    "    def act(self, state: np.ndarray, eps: float = 0.0) -> int:\n",
    "\n",
    "        chosen_action, log_p = self.mc_network.get_action(state)\n",
    "\n",
    "        return chosen_action, log_p\n",
    "        \n",
    "    def step(self, state: np.ndarray, action: int, reward: float, log_prob: float, next_state: np.ndarray, done: bool):\n",
    "        \n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "    ### Implement baseline here, TD(0) updates\n",
    "        if self.baseline:\n",
    "            print(\"implement this\")\n",
    "\n",
    "        if done:\n",
    "            self.learn(gamma = 0.99)\n",
    "            return\n",
    "\n",
    "    def learn(self, gamma: float):\n",
    "\n",
    "        # Computing G value at each time step\n",
    "        G_t = [0] * len(self.log_probs) \n",
    "        val = 0\n",
    "        for i in range(len(self.log_probs)-1, -1, -1):\n",
    "            val *= gamma \n",
    "            val += self.rewards_list[i]\n",
    "            G_t[i] = val\n",
    "\n",
    "        # From list to a tensor\n",
    "        G_t = torch.Tensor(G_t)\n",
    "        # G_t = (G_t - G_t.mean()) / (G_t.std() + 1e-9)\n",
    "\n",
    "    ### Update this\n",
    "        if self.baseline:\n",
    "            G_t -= 0\n",
    "\n",
    "        gradient = []\n",
    "\n",
    "        for i in range(len(self.log_probs)):\n",
    "            gradient.append(-G_t[i] * self.log_probs[i])\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        gradient = torch.stack(gradient).sum()\n",
    "        gradient.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # for param in self.mc_network.parameters():\n",
    "        #     param.grad.data.clamp_(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Trainer(MC_Agent):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int, baseline: bool):\n",
    "        super(MC_Trainer, self).__init__(state_size, action_size, seed, baseline)\n",
    "\n",
    "    def train(self, n_episodes: int = 1000, max_t: int = 1000, eps_start: float = 1.0, eps_end: float = 0.01,eps_decay: float = 0.995):\n",
    "        scores_window = deque(maxlen=100)\n",
    "        scores_list = []\n",
    "        eps = eps_start\n",
    "\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            score = 0\n",
    "            self.log_probs.clear()\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action, log_p = self.act(state, eps)\n",
    "                # print('action is ', action)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                score += reward\n",
    "                \n",
    "                self.step(state, action, reward, log_p, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "            scores_window.append(score)\n",
    "            scores_list.append(score)\n",
    "\n",
    "            eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window) >= 195.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(self.mc_network.state_dict(), 'checkpoint_mc.pth')\n",
    "                break\n",
    "        \n",
    "        return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 29.21\n",
      "Episode 200\tAverage Score: 84.45\n",
      "Episode 300\tAverage Score: 187.95\n",
      "Episode 307\tAverage Score: 195.90\n",
      "Environment solved in 207 episodes!\tAverage Score: 195.90\n"
     ]
    }
   ],
   "source": [
    "trainer = MC_Trainer(state_shape, n_actions, seed=0, baseline=False)\n",
    "scores = trainer.train()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13.0,\n",
       " 14.0,\n",
       " 18.0,\n",
       " 21.0,\n",
       " 14.0,\n",
       " 36.0,\n",
       " 21.0,\n",
       " 48.0,\n",
       " 80.0,\n",
       " 26.0,\n",
       " 13.0,\n",
       " 22.0,\n",
       " 14.0,\n",
       " 52.0,\n",
       " 43.0,\n",
       " 12.0,\n",
       " 13.0,\n",
       " 31.0,\n",
       " 35.0,\n",
       " 21.0,\n",
       " 47.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 22.0,\n",
       " 22.0,\n",
       " 11.0,\n",
       " 24.0,\n",
       " 25.0,\n",
       " 19.0,\n",
       " 23.0,\n",
       " 11.0,\n",
       " 20.0,\n",
       " 47.0,\n",
       " 15.0,\n",
       " 14.0,\n",
       " 30.0,\n",
       " 45.0,\n",
       " 21.0,\n",
       " 23.0,\n",
       " 17.0,\n",
       " 16.0,\n",
       " 17.0,\n",
       " 20.0,\n",
       " 15.0,\n",
       " 16.0,\n",
       " 22.0,\n",
       " 15.0,\n",
       " 19.0,\n",
       " 102.0,\n",
       " 19.0,\n",
       " 32.0,\n",
       " 14.0,\n",
       " 9.0,\n",
       " 15.0,\n",
       " 12.0,\n",
       " 35.0,\n",
       " 16.0,\n",
       " 105.0,\n",
       " 30.0,\n",
       " 26.0,\n",
       " 26.0,\n",
       " 24.0,\n",
       " 69.0,\n",
       " 14.0,\n",
       " 17.0,\n",
       " 50.0,\n",
       " 52.0,\n",
       " 25.0,\n",
       " 16.0,\n",
       " 46.0,\n",
       " 18.0,\n",
       " 26.0,\n",
       " 66.0,\n",
       " 13.0,\n",
       " 17.0,\n",
       " 47.0,\n",
       " 23.0,\n",
       " 18.0,\n",
       " 19.0,\n",
       " 30.0,\n",
       " 28.0,\n",
       " 19.0,\n",
       " 41.0,\n",
       " 17.0,\n",
       " 47.0,\n",
       " 38.0,\n",
       " 30.0,\n",
       " 32.0,\n",
       " 29.0,\n",
       " 57.0,\n",
       " 22.0,\n",
       " 44.0,\n",
       " 29.0,\n",
       " 34.0,\n",
       " 31.0,\n",
       " 40.0,\n",
       " 34.0,\n",
       " 37.0,\n",
       " 97.0,\n",
       " 31.0,\n",
       " 75.0,\n",
       " 16.0,\n",
       " 74.0,\n",
       " 21.0,\n",
       " 34.0,\n",
       " 106.0,\n",
       " 33.0,\n",
       " 20.0,\n",
       " 19.0,\n",
       " 76.0,\n",
       " 106.0,\n",
       " 41.0,\n",
       " 37.0,\n",
       " 84.0,\n",
       " 56.0,\n",
       " 24.0,\n",
       " 57.0,\n",
       " 75.0,\n",
       " 24.0,\n",
       " 81.0,\n",
       " 67.0,\n",
       " 25.0,\n",
       " 42.0,\n",
       " 22.0,\n",
       " 33.0,\n",
       " 20.0,\n",
       " 163.0,\n",
       " 56.0,\n",
       " 100.0,\n",
       " 56.0,\n",
       " 48.0,\n",
       " 62.0,\n",
       " 28.0,\n",
       " 76.0,\n",
       " 44.0,\n",
       " 125.0,\n",
       " 108.0,\n",
       " 22.0,\n",
       " 68.0,\n",
       " 137.0,\n",
       " 62.0,\n",
       " 78.0,\n",
       " 90.0,\n",
       " 160.0,\n",
       " 111.0,\n",
       " 23.0,\n",
       " 110.0,\n",
       " 59.0,\n",
       " 71.0,\n",
       " 66.0,\n",
       " 41.0,\n",
       " 62.0,\n",
       " 40.0,\n",
       " 19.0,\n",
       " 178.0,\n",
       " 45.0,\n",
       " 138.0,\n",
       " 45.0,\n",
       " 51.0,\n",
       " 97.0,\n",
       " 74.0,\n",
       " 98.0,\n",
       " 126.0,\n",
       " 102.0,\n",
       " 201.0,\n",
       " 138.0,\n",
       " 105.0,\n",
       " 200.0,\n",
       " 147.0,\n",
       " 41.0,\n",
       " 91.0,\n",
       " 203.0,\n",
       " 184.0,\n",
       " 120.0,\n",
       " 123.0,\n",
       " 96.0,\n",
       " 69.0,\n",
       " 305.0,\n",
       " 69.0,\n",
       " 126.0,\n",
       " 102.0,\n",
       " 79.0,\n",
       " 107.0,\n",
       " 198.0,\n",
       " 28.0,\n",
       " 104.0,\n",
       " 39.0,\n",
       " 81.0,\n",
       " 92.0,\n",
       " 43.0,\n",
       " 168.0,\n",
       " 96.0,\n",
       " 103.0,\n",
       " 71.0,\n",
       " 59.0,\n",
       " 161.0,\n",
       " 127.0,\n",
       " 95.0,\n",
       " 34.0,\n",
       " 133.0,\n",
       " 175.0,\n",
       " 72.0,\n",
       " 41.0,\n",
       " 117.0,\n",
       " 193.0,\n",
       " 251.0,\n",
       " 59.0,\n",
       " 172.0,\n",
       " 165.0,\n",
       " 204.0,\n",
       " 267.0,\n",
       " 165.0,\n",
       " 67.0,\n",
       " 133.0,\n",
       " 244.0,\n",
       " 291.0,\n",
       " 186.0,\n",
       " 52.0,\n",
       " 153.0,\n",
       " 53.0,\n",
       " 191.0,\n",
       " 203.0,\n",
       " 124.0,\n",
       " 104.0,\n",
       " 151.0,\n",
       " 27.0,\n",
       " 73.0,\n",
       " 135.0,\n",
       " 110.0,\n",
       " 47.0,\n",
       " 129.0,\n",
       " 61.0,\n",
       " 33.0,\n",
       " 119.0,\n",
       " 101.0,\n",
       " 158.0,\n",
       " 51.0,\n",
       " 98.0,\n",
       " 268.0,\n",
       " 231.0,\n",
       " 30.0,\n",
       " 83.0,\n",
       " 160.0,\n",
       " 200.0,\n",
       " 206.0,\n",
       " 190.0,\n",
       " 150.0,\n",
       " 219.0,\n",
       " 262.0,\n",
       " 69.0,\n",
       " 262.0,\n",
       " 189.0,\n",
       " 329.0,\n",
       " 257.0,\n",
       " 338.0,\n",
       " 327.0,\n",
       " 223.0,\n",
       " 124.0,\n",
       " 73.0,\n",
       " 106.0,\n",
       " 228.0,\n",
       " 173.0,\n",
       " 190.0,\n",
       " 364.0,\n",
       " 205.0,\n",
       " 212.0,\n",
       " 376.0,\n",
       " 222.0,\n",
       " 43.0,\n",
       " 237.0,\n",
       " 265.0,\n",
       " 222.0,\n",
       " 150.0,\n",
       " 274.0,\n",
       " 168.0,\n",
       " 280.0,\n",
       " 436.0,\n",
       " 396.0,\n",
       " 141.0,\n",
       " 146.0,\n",
       " 24.0,\n",
       " 288.0,\n",
       " 167.0,\n",
       " 215.0,\n",
       " 180.0,\n",
       " 285.0,\n",
       " 153.0,\n",
       " 252.0,\n",
       " 222.0,\n",
       " 110.0,\n",
       " 252.0,\n",
       " 158.0,\n",
       " 155.0,\n",
       " 399.0,\n",
       " 232.0,\n",
       " 453.0,\n",
       " 258.0,\n",
       " 294.0,\n",
       " 209.0,\n",
       " 490.0,\n",
       " 174.0,\n",
       " 189.0,\n",
       " 366.0,\n",
       " 198.0,\n",
       " 340.0,\n",
       " 194.0,\n",
       " 242.0]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
