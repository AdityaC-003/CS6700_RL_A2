{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 0.0005\n",
    "UPDATE_EVERY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n      \n",
    "\n",
    "state = env.reset(seed=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.value_layer = nn.Linear(fc2_units, 1)\n",
    "        self.advantage_layer = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork1(QNetwork):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork1, self).__init__(\n",
    "            state_size, action_size, seed, fc1_units, fc2_units\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        value = self.value_layer(x)\n",
    "        advantage = self.advantage_layer(x)\n",
    "\n",
    "        q = value + (advantage - advantage.mean())\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork2(QNetwork):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork2, self).__init__(\n",
    "            state_size, action_size, seed, fc1_units, fc2_units\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        value = self.value_layer(x)\n",
    "        advantage = self.advantage_layer(x)\n",
    "\n",
    "        q = value + (advantage - advantage.max())\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int):\n",
    "        \"\"\"Agent-Environment Interaction\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        \"\"\" Q-Network \"\"\"\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        \"\"\" Replay memory\"\"\"\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        \"\"\" Initialize time step (for updating every UPDATE_EVERY steps) \"\"\"\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"Save experience in replay memory\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        \"\"\" If enough samples are available in memory, get random subset and learn \"\"\"\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" Update the target network every UPDATE_EVERY steps \"\"\"\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state: np.ndarray, eps: float = 0.0) -> int:\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        \"\"\" Epsilon-greedy action selection \"\"\"\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences: namedtuple, gamma: float):\n",
    "        \"\"\"+E EXPERIENCE REPLAY PRESENT\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \"\"\" Get max predicted Q values (for next states) from target model \"\"\"\n",
    "        Q_targets_next = (\n",
    "            self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        \"\"\" Compute Q targets for current states \"\"\"\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        \"\"\" Get expected Q values from local model \"\"\"\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        \"\"\" Compute loss \"\"\"\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        \"\"\" Minimize the loss \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        \"\"\" Gradient Clipping \"\"\"\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            # check if some parameter has gradient none, if so print the name of the parameter\n",
    "            if param.grad is None:\n",
    "                print(param.names, \"has gradient None\", param.shape)\n",
    "            \n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Agent):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int):\n",
    "        super(Trainer, self).__init__(state_size, action_size, seed)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        n_episodes: int = 2000,\n",
    "        max_t: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        eps_decay: float = 0.995,\n",
    "    ):\n",
    "        scores_window = deque(maxlen=100)\n",
    "        scores_list = []\n",
    "        eps = eps_start\n",
    "\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            score = 0\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                action = self.act(state, eps)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                self.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "            scores_window.append(score)\n",
    "            scores_list.append(score)\n",
    "\n",
    "            eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window) >= 195.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode - 100, np.mean(scores_window)))\n",
    "                torch.save(self.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "        \n",
    "        return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6\tAverage Score: 16.83"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 46.94\n",
      "Episode 200\tAverage Score: 165.33\n",
      "Episode 221\tAverage Score: 196.96\n",
      "Environment solved in 121 episodes!\tAverage Score: 196.96\n",
      "Time Elapsed:  0:00:52.439880\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "trainer = Trainer(state_shape, n_actions, seed=0)\n",
    "scores = trainer.train()\n",
    "\n",
    "time_elapsed = datetime.datetime.now() - begin_time\n",
    "\n",
    "print('Time Elapsed: ', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCRNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int = 0, fc1_units: int = 128, fc2_units: int = 64):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(MCRNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "         \n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # self.output_layer = nn.Linear(fc2_units, action_size)\n",
    "        self.output_layer = nn.Linear(fc1_units, action_size)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(state))\n",
    "        \n",
    "        # x = F.relu(self.fc2(x))\n",
    "\n",
    "        output = F.softmax(self.output_layer(x), dim = 1)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_action(self, state): \n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        action_values = self.forward(state)\n",
    "        chosen_action = random.choice(np.arange(self.action_size))\n",
    "        log_p = torch.log(action_values.squeeze(0)[chosen_action])\n",
    "\n",
    "        return chosen_action, log_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent for Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Agent:\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int, baseline: bool):\n",
    "        \"\"\"Agent-Environment Interaction\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        \"\"\" MC-network \"\"\"\n",
    "        self.mc_network = MCRNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.mc_network.parameters(), lr=LR)\n",
    "\n",
    "        \"\"\" Initialize time step (for updating every UPDATE_EVERY steps) \"\"\"\n",
    "        self.t_step = 0\n",
    "\n",
    "        \"\"\" Collecing log of probabilities for gradient descent update \"\"\"\n",
    "        self.log_probs = []\n",
    "\n",
    "        \"\"\" Storing rewards for updates \"\"\"\n",
    "        self.rewards_list = []\n",
    "\n",
    "        \"\"\" Baseline flag, if true, normalize with Value function obtained by TD(0) \"\"\"\n",
    "        self.baseline = baseline\n",
    "        \n",
    "    def act(self, state: np.ndarray, eps: float = 0.0) -> int:\n",
    "\n",
    "        chosen_action, log_p = self.mc_network.get_action(state)\n",
    "\n",
    "        return chosen_action, log_p\n",
    "        \n",
    "    def step(self, state: np.ndarray, action: int, reward: float, log_prob: float, next_state: np.ndarray, done: bool):\n",
    "        \n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "    ### Implement baseline here, TD(0) updates\n",
    "        if self.baseline:\n",
    "            print(\"implement this\")\n",
    "\n",
    "        if done:\n",
    "            self.learn(gamma = 0.99)\n",
    "            return\n",
    "\n",
    "    def learn(self, gamma: float):\n",
    "\n",
    "        # Computing G value at each time step\n",
    "        G_t = [0] * len(self.log_probs) \n",
    "        val = 0\n",
    "        for i in range(len(self.log_probs)-1, -1, -1):\n",
    "            val *= gamma \n",
    "            val += self.rewards_list[i]\n",
    "            G_t[i] = val\n",
    "\n",
    "        # From list to a tensor\n",
    "        G_t = torch.Tensor(G_t)\n",
    "\n",
    "    ### Update this\n",
    "        if self.baseline:\n",
    "            G_t -= 0\n",
    "\n",
    "        gradient = []\n",
    "\n",
    "        for i in range(len(self.log_probs)):\n",
    "            gradient.append(-G_t[i] * self.log_probs[i])\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # gradient = torch.stack(gradient).sum()\n",
    "        # print(gradient)\n",
    "        # gradient = torch.Tensor(gradient, requires_grad)\n",
    "        gradient = torch.stack(gradient).sum()\n",
    "        print('grad', gradient)\n",
    "\n",
    "        gradient.backward()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "\n",
    "        for param in self.mc_network.parameters():\n",
    "            if param.grad is None:\n",
    "                print(param.names, \"has gradient None\", param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Trainer(MC_Agent):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int, baseline: bool):\n",
    "        super(MC_Trainer, self).__init__(state_size, action_size, seed, baseline)\n",
    "\n",
    "    def train(self, n_episodes: int = 20, max_t: int = 1000, eps_start: float = 1.0, eps_end: float = 0.01,eps_decay: float = 0.995):\n",
    "        scores_window = deque(maxlen=100)\n",
    "        scores_list = []\n",
    "        eps = eps_start\n",
    "\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            score = 0\n",
    "            self.log_probs.clear()\n",
    "\n",
    "            for t in range(max_t):\n",
    "                action, log_p = self.act(state, eps)\n",
    "                # print('action is ', action)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                score += reward\n",
    "                \n",
    "                self.step(state, action, reward, log_p, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "            scores_window.append(score)\n",
    "            scores_list.append(score)\n",
    "\n",
    "            eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window) >= 195.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode - 100, np.mean(scores_window)))\n",
    "                torch.save(self.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "        \n",
    "        return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad tensor(44.1293, grad_fn=<SumBackward0>)\n",
      "Episode 1\tAverage Score: 11.00grad tensor(916.0723, grad_fn=<SumBackward0>)\n",
      "Episode 2\tAverage Score: 33.00grad tensor(59.7395, grad_fn=<SumBackward0>)\n",
      "Episode 3\tAverage Score: 26.33grad tensor(474.1797, grad_fn=<SumBackward0>)\n",
      "Episode 4\tAverage Score: 29.50grad tensor(90.3680, grad_fn=<SumBackward0>)\n",
      "Episode 5\tAverage Score: 26.80grad tensor(531.8133, grad_fn=<SumBackward0>)\n",
      "Episode 6\tAverage Score: 29.17grad tensor(462.9434, grad_fn=<SumBackward0>)\n",
      "Episode 7\tAverage Score: 30.43grad tensor(60.4035, grad_fn=<SumBackward0>)\n",
      "Episode 8\tAverage Score: 28.25grad tensor(794.3004, grad_fn=<SumBackward0>)\n",
      "Episode 9\tAverage Score: 30.78grad tensor(61.4827, grad_fn=<SumBackward0>)\n",
      "Episode 10\tAverage Score: 29.00grad tensor(679.4032, grad_fn=<SumBackward0>)\n",
      "Episode 11\tAverage Score: 30.64grad tensor(80.3026, grad_fn=<SumBackward0>)\n",
      "Episode 12\tAverage Score: 29.33grad tensor(436.6233, grad_fn=<SumBackward0>)\n",
      "Episode 13\tAverage Score: 29.92grad tensor(70.3981, grad_fn=<SumBackward0>)\n",
      "Episode 14\tAverage Score: 28.79grad tensor(241.7152, grad_fn=<SumBackward0>)\n",
      "Episode 15\tAverage Score: 28.67grad tensor(178.8009, grad_fn=<SumBackward0>)\n",
      "Episode 16\tAverage Score: 28.31grad tensor(89.5151, grad_fn=<SumBackward0>)\n",
      "Episode 17\tAverage Score: 27.59grad tensor(349.6492, grad_fn=<SumBackward0>)\n",
      "Episode 18\tAverage Score: 27.89grad tensor(100.8282, grad_fn=<SumBackward0>)\n",
      "Episode 19\tAverage Score: 27.32grad tensor(192.0687, grad_fn=<SumBackward0>)\n",
      "Episode 20\tAverage Score: 27.15"
     ]
    }
   ],
   "source": [
    "trainer = MC_Trainer(state_shape, n_actions, seed=0, baseline=False)\n",
    "scores = trainer.train()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.0,\n",
       " 55.0,\n",
       " 13.0,\n",
       " 39.0,\n",
       " 16.0,\n",
       " 41.0,\n",
       " 38.0,\n",
       " 13.0,\n",
       " 51.0,\n",
       " 13.0,\n",
       " 47.0,\n",
       " 15.0,\n",
       " 37.0,\n",
       " 14.0,\n",
       " 27.0,\n",
       " 23.0,\n",
       " 16.0,\n",
       " 33.0,\n",
       " 17.0,\n",
       " 24.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
