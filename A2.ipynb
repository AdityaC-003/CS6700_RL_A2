{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 0.0005\n",
    "UPDATE_EVERY = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n      \n",
    "\n",
    "state = env.reset(seed=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.value_layer = nn.Linear(fc2_units, 1)\n",
    "        self.advantage_layer = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork1(QNetwork):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork1, self).__init__(\n",
    "            state_size, action_size, seed, fc1_units, fc2_units\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        value = self.value_layer(x)\n",
    "        advantage = self.advantage_layer(x)\n",
    "\n",
    "        q = value + (advantage - advantage.mean())\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork2(QNetwork):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        seed: int = 0,\n",
    "        fc1_units: int = 128,\n",
    "        fc2_units: int = 64,\n",
    "    ):\n",
    "        \"\"\"Initialize parameters and build model.\"\"\"\n",
    "        super(QNetwork1, self).__init__(\n",
    "            state_size, action_size, seed, fc1_units, fc2_units\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        value = self.value_layer(x)\n",
    "        advantage = self.advantage_layer(x)\n",
    "\n",
    "        q = value + (advantage - advantage.max())\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int):\n",
    "        \"\"\"Agent-Environment Interaction\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        \"\"\" Q-Network \"\"\"\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        \"\"\" Replay memory\"\"\"\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        \"\"\" Initialize time step (for updating every UPDATE_EVERY steps) \"\"\"\n",
    "        self.t_step = 0\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "    ):\n",
    "        \"\"\"Save experience in replay memory\"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        \"\"\" If enough samples are available in memory, get random subset and learn \"\"\"\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" Update the target network every UPDATE_EVERY steps \"\"\"\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state: np.ndarray, eps: float = 0.0) -> int:\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        \"\"\" Epsilon-greedy action selection \"\"\"\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences: namedtuple, gamma: float):\n",
    "        \"\"\"+E EXPERIENCE REPLAY PRESENT\"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \"\"\" Get max predicted Q values (for next states) from target model \"\"\"\n",
    "        Q_targets_next = (\n",
    "            self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        )\n",
    "\n",
    "        \"\"\" Compute Q targets for current states \"\"\"\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        \"\"\" Get expected Q values from local model \"\"\"\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        \"\"\" Compute loss \"\"\"\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        \"\"\" Minimize the loss \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        \"\"\" Gradient Clipping \"\"\"\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            # check if some parameter has gradient none, if so print the name of the parameter\n",
    "            if param.grad is None:\n",
    "                print(param.names, \"has gradient None\", param.shape)\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Agent):\n",
    "\n",
    "    def __init__(self, state_size: int, action_size: int, seed: int):\n",
    "        super(Trainer, self).__init__(state_size, action_size, seed)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        n_episodes: int = 2000,\n",
    "        max_t: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        eps_decay: float = 0.995,\n",
    "    ):\n",
    "        scores_window = deque(maxlen=100)\n",
    "        scores_list = []\n",
    "        eps = eps_start\n",
    "\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            state, _ = env.reset()\n",
    "            score = 0\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                action = self.act(state, eps)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                self.step(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "        \n",
    "            scores_window.append(score)\n",
    "            scores_list.append(score)\n",
    "\n",
    "            eps = max(eps_end, eps_decay * eps)\n",
    "\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            \n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            if np.mean(scores_window) >= 195.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode - 100, np.mean(scores_window)))\n",
    "                torch.save(self.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "        \n",
    "        return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7\tAverage Score: 16.86"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 46.94\n",
      "Episode 200\tAverage Score: 165.33\n",
      "Episode 221\tAverage Score: 196.96\n",
      "Environment solved in 121 episodes!\tAverage Score: 196.96\n",
      "Time Elapsed:  0:00:53.058109\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "trainer = Trainer(state_shape, n_actions, seed=0)\n",
    "scores = trainer.train()\n",
    "\n",
    "time_elapsed = datetime.datetime.now() - begin_time\n",
    "\n",
    "print('Time Elapsed: ', time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
